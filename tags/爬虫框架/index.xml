<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>爬虫框架 on Lao&#39;s Blog</title>
    <link>https://laoooo.cn/tags/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/</link>
    <description>Recent content in 爬虫框架 on Lao&#39;s Blog</description>
    <image>
      <url>https://laoooo.cn/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://laoooo.cn/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 26 Nov 2021 11:03:01 +0000</lastBuildDate><atom:link href="https://laoooo.cn/tags/%E7%88%AC%E8%99%AB%E6%A1%86%E6%9E%B6/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Scrapy 框架学习记录（豆瓣）</title>
      <link>https://laoooo.cn/scrapy-%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E8%B1%86%E7%93%A3/</link>
      <pubDate>Fri, 26 Nov 2021 11:03:01 +0000</pubDate>
      
      <guid>https://laoooo.cn/scrapy-%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95%E8%B1%86%E7%93%A3/</guid>
      <description>Scrapy 豆瓣爬虫 安装 下列的安装步骤假定您已经安装好下列程序: Python 2.7或以上版本 Python Package: pip and setuptools. 现在 pip 依赖 setuptools ，如果未安装，则会自动安装 setuptools 。 lxml. 大多数Linu</description>
      <content:encoded><![CDATA[<h1 id="scrapy-豆瓣爬虫">Scrapy 豆瓣爬虫</h1>
<h2 id="安装">安装</h2>
<p>下列的安装步骤假定您已经安装好下列程序:</p>
<ul>
<li><a href="http://www.python.org/">Python</a> 2.7或以上版本</li>
<li>Python Package: <a href="http://www.pip-installer.org/en/latest/installing.html">pip</a> and <a href="https://pypi.python.org/pypi/setuptools">setuptools</a>. 现在 <a href="http://www.pip-installer.org/en/latest/installing.html">pip</a> 依赖 <a href="https://pypi.python.org/pypi/setuptools">setuptools</a> ，如果未安装，则会自动安装 <a href="https://pypi.python.org/pypi/setuptools">setuptools</a> 。</li>
<li><a href="http://lxml.de/">lxml</a>. 大多数Linux发行版自带了lxml。如果缺失，请查看http://lxml.de/installation.html</li>
<li><a href="https://pypi.python.org/pypi/pyOpenSSL">OpenSSL</a>. 除了Windows(请查看 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/install.html#intro-install-platform-notes">平台安装指南</a>)之外的系统都已经提供。</li>
</ul>
<p>您可以使用pip来安装Scrapy(推荐使用pip来安装Python package).</p>
<p>使用pip安装:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install Scrapy
</span></span></code></pre></div><h2 id="创建项目">创建项目</h2>
<p>创建一个scrapy爬虫项目和一般的python框架一样，使用以下命令即可快速创建一个项目：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scrapy startproject 项目名称
</span></span></code></pre></div><p>该命令成功执行后，应包含如下目录：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">doodlexu@MacBook tutorial % tree
</span></span><span class="line"><span class="cl">.
</span></span><span class="line"><span class="cl">├── scrapy.cfg
</span></span><span class="line"><span class="cl">└── tutorial
</span></span><span class="line"><span class="cl">    ├── __init__.py
</span></span><span class="line"><span class="cl">    ├── items.py
</span></span><span class="line"><span class="cl">    ├── middlewares.py
</span></span><span class="line"><span class="cl">    ├── pipelines.py
</span></span><span class="line"><span class="cl">    ├── settings.py
</span></span><span class="line"><span class="cl">    └── spiders
</span></span><span class="line"><span class="cl">        └── __init__.py
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="m">2</span> directories, <span class="m">7</span> files
</span></span></code></pre></div><p>这些文件分别是:</p>
<ul>
<li><code>scrapy.cfg</code>: 项目的配置文件</li>
<li><code>tutorial/</code>: 该项目的python模块。之后您将在此加入代码。</li>
<li><code>tutorial/items.py</code>: 项目中的item文件.</li>
<li><code>tutorial/pipelines.py</code>: 项目中的pipelines文件.</li>
<li><code>tutorial/settings.py</code>: 项目的设置文件.</li>
<li><code>tutorial/spiders/</code>: 放置spider代码的目录.</li>
</ul>
<h2 id="定义item">定义Item</h2>
<blockquote>
<p>Item是爬取到的数据的容器，他的语法很简单，和python字典（dict）类似，并且提供了额外保护机制来避免拼写错误导致的未定义字段错误。</p>
</blockquote>
<p>类似在ORM中做的一样，您可以通过创建一个 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/items.html#scrapy.item.Item"><code>scrapy.Item</code></a>类， 并且定义类型为 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/items.html#scrapy.item.Field"><code>scrapy.Field</code></a> 的类属性来定义一个Item。其写法如下：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scrapy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">douban_shortItem</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Item</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">votes</span>   <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>  <span class="c1"># 有用投票</span>
</span></span><span class="line"><span class="cl">    <span class="nb">id</span>      <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>  <span class="c1"># 用户ID</span>
</span></span><span class="line"><span class="cl">    <span class="n">rating</span>  <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>  <span class="c1"># 用户打分</span>
</span></span><span class="line"><span class="cl">    <span class="n">date</span>    <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>  <span class="c1"># 短评日期</span>
</span></span><span class="line"><span class="cl">    <span class="n">content</span> <span class="o">=</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Field</span><span class="p">()</span>  <span class="c1"># 短评内容</span>
</span></span></code></pre></div><h2 id="编写爬虫">编写爬虫</h2>
<p>Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</p>
<p>其包含了一个用于下载的初始URL，如何跟进网页中的链接以及如何分析页面中的内容， 提取生成 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/items.html#topics-items">item</a> 的方法。</p>
<p>为了创建一个Spider，您必须继承 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider"><code>scrapy.Spider</code></a> 类， 且定义以下三个属性:</p>
<ul>
<li><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.name"><code>name</code></a>: 用于区别Spider。 该名字必须是唯一的，您不可以为不同的Spider设定相同的名字。</li>
<li><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.start_urls"><code>start_urls</code></a>: 包含了Spider在启动时进行爬取的url列表。 因此，第一个被获取到的页面将是其中之一。 后续的URL则从初始的URL获取到的数据中提取。</li>
<li><a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/spiders.html#scrapy.spider.Spider.parse"><code>parse()</code></a> 是spider的一个方法。 被调用时，每个初始URL完成下载后生成的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html#scrapy.http.Response"><code>Response</code></a> 对象将会作为唯一的参数传递给该函数。 该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 <a href="https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/request-response.html#scrapy.http.Request"><code>Request</code></a> 对象。</li>
</ul>
<p>以下为我们的Spider代码，它的目标是爬取电影（毒液2）的首页短评，并保存。代码需放在 <code>tutorial/spiders</code> 目录下的 <code>douban_short.py</code> 文件中:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">scrapy</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mySpider.items</span> <span class="kn">import</span> <span class="n">douban_shortItem</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">DoubanShortSpider</span><span class="p">(</span><span class="n">scrapy</span><span class="o">.</span><span class="n">Spider</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;douban_short&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="n">allowed_domains</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;douban.com&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;User-Agent&#39;</span><span class="p">:</span> <span class="s1">&#39;Mozilla/5.0 (Macintosh Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.54 Safari/537.36&#39;</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="n">cookies</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">        <span class="s1">&#39;your_cookies&#39;</span> <span class="o">=</span> <span class="s1">&#39;your_values&#39;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">start_requests</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;https://movie.douban.com/subject/30382416/comments&#39;</span>
</span></span><span class="line"><span class="cl">        <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">,</span> <span class="n">cookies</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cookies</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="k">def</span> <span class="nf">parse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">item</span> <span class="o">=</span> <span class="n">douban_shortItem</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="n">shorts</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//div[@id=&#34;comments&#34;]/div&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">for</span> <span class="n">short</span> <span class="ow">in</span> <span class="n">shorts</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;votes&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">short</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;.//div[2]/h3/span[1]/span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">short</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;.//div[2]/h3/span[2]/a/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;rating&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">short</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;.//div[2]/h3/span[2]/span[2]/@class&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">re</span><span class="p">(</span><span class="sa">u</span><span class="s1">&#39;\d&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">short</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;.//div[2]/h3/span[2]/span[3]/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">                <span class="n">item</span><span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">short</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">                    <span class="s1">&#39;.//div[2]/p/span/text()&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">                <span class="k">yield</span> <span class="n">item</span>
</span></span><span class="line"><span class="cl">            <span class="k">except</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">                <span class="k">pass</span>
</span></span></code></pre></div><p>在爬取豆瓣的过程中，我们需要使用到cookies这项技术，它可以帮助服务器认识你是谁，给我们的小蜘蛛提供一层掩护，这样他就不会拒绝提供数据了。</p>
<p>另一个需要注意的点就是我们的headers，它的作用也和cookies类似，不过它不具备标识到个人，而是标识你用的是什么类型的浏览器和设备，以确保能正确解码。</p>
<p>这两项参数都可以在浏览器的开发者工具中找到，相信聪明的你一定可以的🤪</p>
<h3 id="自动翻页">自动翻页</h3>
<p>看到这，你肯定也发现了，到目前为止，这个小蜘蛛虽然干得很不错，顺利的拿到了数据，但还不够，我们需要所有数据。但是豆瓣为了保证用户体验，做了分页处理，这个时候要怎么办呢？</p>
<blockquote>
<p>你已经是个成熟的🕷️了，你得学会自己翻页</p>
</blockquote>
<p>回想一下，我们正常通过浏览器访问的时候，看下一页是通过点击次页按钮来实现的，那么我们的蜘蛛也可以这么做。我们可以通过xpath来定位到下一页按钮的目标的URL，并且只要它存在，我们就继续放🕷️：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">next_url</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">xpath</span><span class="p">(</span><span class="s1">&#39;//*[@id=&#34;paginator&#34;]/a[@class=&#34;next&#34;]/@href&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">extract</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">next_url</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">next_url</span> <span class="o">=</span> <span class="s1">&#39;https://movie.douban.com/subject/30382416/comments&#39;</span> <span class="o">+</span> <span class="n">next_url</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">            <span class="k">yield</span> <span class="n">scrapy</span><span class="o">.</span><span class="n">Request</span><span class="p">(</span><span class="n">next_url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">headers</span><span class="p">,</span> <span class="n">cookies</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">cookies</span><span class="p">)</span>
</span></span></code></pre></div><p>至此，我们已经可以遍历所有的评论了。</p>
<h2 id="爬取">爬取</h2>
<p>进入项目的根目录，执行下列命令启动spider:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scrapy crawl douban_short
</span></span></code></pre></div><p>命令执行后会有一大堆<code>调试</code>日志输出，别紧张，这是正常的。你可以在setting.py里随意调整日志输出级别：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">logging</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="n">LOG_LEVEL</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">DEBUG</span> <span class="c1"># 日志输出级别</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span></code></pre></div><h3 id="保存">保存</h3>
<p>执行上面命令会爬，但不会存，我们需要利用一个<code>-O</code>的参数来指定保存的文件名以及格式，比如像这样：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">scrapy crawl douban_short -O shorts.csv
</span></span></code></pre></div><p>他会将保存到item里面的数据保存到shorts.cvs文件里，而且scrapy会自动辨别你的文件格式，自动的解析并保存。它支持的格式有：</p>
<ul>
<li>json</li>
<li>jsonlines</li>
<li>jl</li>
<li>csv</li>
<li>xml</li>
<li>marshal</li>
<li>pickle</li>
</ul>
]]></content:encoded>
    </item>
    
  </channel>
</rss>
